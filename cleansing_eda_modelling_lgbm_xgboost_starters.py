# -*- coding: utf-8 -*-
"""cleansing-eda-modelling-lgbm-xgboost-starters.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YTd1I4HE54dGuWG03ZkLXEbu22-UIcKM
"""

import numpy as np
import pandas as pd
import sklearn
import seaborn as sns
import matplotlib.pyplot as plt

fare_prediction_data = pd.read_csv("/content/fare_prediction_data.csv")

fare_prediction_data.head(1)

"""**PART 1 --> DATA CLEANSING & EXPLORATORY DATA ANALYSIS (EDA)**

Will perform the following activities
* Shape of data
* Check for NaNs and drop them (if any)
* Check for outliers and drop them (if any)
* Type conversion of relevant fields
"""

fare_prediction_data.shape

fare_prediction_data.describe()

#check for missing values in train data
fare_prediction_data.isnull().sum().sort_values(ascending=False)

#drop the missing values
fare_prediction_data = fare_prediction_data.drop(fare_prediction_data[fare_prediction_data.isnull().any(1)].index, axis = 0)

fare_prediction_data.shape

#check the target column
fare_prediction_data['fare_amount'].describe()

"""Fare amount has a negative value, which doesn't make sense. Remove these fields"""

#38 fields have negative fare_amount values.
from collections import Counter
Counter(fare_prediction_data['fare_amount']<0)

fare_prediction_data = fare_prediction_data.drop(fare_prediction_data[fare_prediction_data['fare_amount']<0].index, axis=0)
fare_prediction_data.shape

#no more negative values in the fare field
fare_prediction_data['fare_amount'].describe()

#highest fare is $500
fare_prediction_data['fare_amount'].sort_values(ascending=False)

"""Next check the passenger_count variable"""

#Max number of passengers are 6. Which makes sense is the cab is an SUV :)
fare_prediction_data['passenger_count'].describe()



#Next, let us explore the pickup latitude and longitudes
fare_prediction_data['pickup_latitude'].describe()

"""* Latitudes range from -90 to 90.
* Longitudes range from -180 to 180.

The above describe clearly shows some outliers. Let's filter them
"""

fare_prediction_data[fare_prediction_data['pickup_latitude']<-90]

fare_prediction_data[fare_prediction_data['pickup_latitude']>90]

#We need to drop these outliers
fare_prediction_data['pickup_latitude'] = fare_prediction_data['pickup_latitude'].astype(float)
fare_prediction_data = fare_prediction_data[(fare_prediction_data['pickup_latitude'] >= -90) & (fare_prediction_data['pickup_latitude'] <= 90)]

#12 rows dropped
fare_prediction_data.shape

#similar operation for pickup longitude
fare_prediction_data['pickup_longitude'].describe()

#similar operation for dropoff latitude and longitude
fare_prediction_data[fare_prediction_data['dropoff_latitude']<-90]

fare_prediction_data[fare_prediction_data['dropoff_latitude']>90]

#We need to drop these outliers
fare_prediction_data['dropoff_latitude'] = fare_prediction_data['dropoff_latitude'].astype(float)
fare_prediction_data = fare_prediction_data[(fare_prediction_data['dropoff_latitude'] >= -90) & (fare_prediction_data['dropoff_latitude'] <= 90)]

#3 rows dropped
fare_prediction_data.shape

"""Check the data types of each column"""

fare_prediction_data.dtypes

"""key and pickup_datetime seem to be datetime columns which are in object format. Let's convert them to datetime"""

fare_prediction_data['key'] = pd.to_datetime(fare_prediction_data['key'])
fare_prediction_data['pickup_datetime']  = pd.to_datetime(fare_prediction_data['pickup_datetime'])

#Convert for test data
fare_prediction_data['key'] = pd.to_datetime(fare_prediction_data['key'])
fare_prediction_data['pickup_datetime']  = pd.to_datetime(fare_prediction_data['pickup_datetime'])

#check the dtypes after conversion
fare_prediction_data.dtypes

"""We can calulate the distance in a sphere when latitudes and longitudes are given by [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula)

**haversine(θ) = sin²(θ/2)**

Eventually, the formual boils down to the following where φ is latitude, λ is longitude, R is earth’s radius (mean radius = 6,371km) to include latitude and longitude coordinates (A and B in this case).

**a = sin²((φB - φA)/2) + cos φA . cos φB . sin²((λB - λA)/2)**

**c = 2 * atan2( √a, √(1−a) )**

**d = R ⋅ c**

**d = Haversine distance**

*Refer [this](https://community.esri.com/groups/coordinate-reference-systems/blog/2017/10/05/haversine-formula) page for more info and examples on Haversine formula*
"""

def haversine_distance(lat1, long1, lat2, long2):
    data = [fare_prediction_data]
    for i in data:
        R = 6371  #radius of earth in kilometers
        #R = 3959 #radius of earth in miles
        phi1 = np.radians(i[lat1])
        phi2 = np.radians(i[lat2])

        delta_phi = np.radians(i[lat2]-i[lat1])
        delta_lambda = np.radians(i[long2]-i[long1])

        #a = sin²((φB - φA)/2) + cos φA . cos φB . sin²((λB - λA)/2)
        a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2

        #c = 2 * atan2( √a, √(1−a) )
        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))

        #d = R*c
        d = (R * c) #in kilometers
        i['H_Distance'] = d
    return d

haversine_distance('pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude')

fare_prediction_data['H_Distance'].head(10)

"""Now that we have calculated the distance, we shall create columns for the following -
* year
* month
* date
* hour
* day of week
"""

data = [fare_prediction_data]
for i in data:
    i['Year'] = i['pickup_datetime'].dt.year
    i['Month'] = i['pickup_datetime'].dt.month
    i['Date'] = i['pickup_datetime'].dt.day
    i['Day of Week'] = i['pickup_datetime'].dt.dayofweek
    i['Hour'] = i['pickup_datetime'].dt.hour

fare_prediction_data.head()

fare_prediction_data.sort_values(['H_Distance','fare_amount'], ascending=False)

len(fare_prediction_data)

bins_0 = fare_prediction_data.loc[(fare_prediction_data['H_Distance'] == 0), ['H_Distance']]
bins_1 = fare_prediction_data.loc[(fare_prediction_data['H_Distance'] > 0) & (fare_prediction_data['H_Distance'] <= 10),['H_Distance']]
bins_2 = fare_prediction_data.loc[(fare_prediction_data['H_Distance'] > 10) & (fare_prediction_data['H_Distance'] <= 50),['H_Distance']]
bins_3 = fare_prediction_data.loc[(fare_prediction_data['H_Distance'] > 50) & (fare_prediction_data['H_Distance'] <= 100),['H_Distance']]
bins_4 = fare_prediction_data.loc[(fare_prediction_data['H_Distance'] > 100) & (fare_prediction_data['H_Distance'] <= 200),['H_Distance']]
bins_5 = fare_prediction_data.loc[(fare_prediction_data['H_Distance'] > 200) & (fare_prediction_data['H_Distance'] <= 300),['H_Distance']]
bins_6 = fare_prediction_data.loc[(fare_prediction_data['H_Distance'] > 300),['H_Distance']]
bins_0['bins'] = '0'
bins_1['bins'] = '0-10'
bins_2['bins'] = '11-50'
bins_3['bins'] = '51-100'
bins_4['bins'] = '100-200'
bins_5['bins'] = '201-300'
bins_6['bins'] = '>300'
dist_bins =pd.concat([bins_0,bins_1,bins_2,bins_3,bins_4,bins_5,bins_6])
#len(dist_bins)
dist_bins.columns

plt.figure(figsize=(15,7))
plt.hist(dist_bins['bins'], bins=75)
plt.xlabel('Bins')
plt.ylabel('Frequency')

Counter(dist_bins['bins'])

"""There are values which are greater than 100 kms! In NYC I am not sure why people would take cabs to travel more than a 100 kms. Since the number of bins for 100-200 kms is quite high, I will keep these. These outliers could be because of typos or missing values in the latitude or longitude. Remove fields of the following -
1.  Pickup latitude and pickup longitude are 0 but dropoff latitude and longitude are not 0, but the fare is 0
2. vice versa of point 1.
3. Pickup latitude and pickup longitude are 0 but dropoff latitude and longitude are not 0, but the fare is NOT 0. Here I will have to impute the distance values in both the train and test data.
"""

#pickup latitude and longitude = 0
fare_prediction_data.loc[((fare_prediction_data['pickup_latitude']==0) & (fare_prediction_data['pickup_longitude']==0))&((fare_prediction_data['dropoff_latitude']!=0) & (fare_prediction_data['dropoff_longitude']!=0)) & (fare_prediction_data['fare_amount']==0)]

fare_prediction_data = fare_prediction_data.drop(fare_prediction_data.loc[((fare_prediction_data['pickup_latitude']==0) & (fare_prediction_data['pickup_longitude']==0))&((fare_prediction_data['dropoff_latitude']!=0) & (fare_prediction_data['dropoff_longitude']!=0)) & (fare_prediction_data['fare_amount']==0)].index, axis=0)

#1 row dropped
fare_prediction_data.shape

#Check in fare_prediction_data data
fare_prediction_data.loc[((fare_prediction_data['pickup_latitude']==0) & (fare_prediction_data['pickup_longitude']==0))&((fare_prediction_data['dropoff_latitude']!=0) & (fare_prediction_data['dropoff_longitude']!=0))]

#dropoff latitude and longitude = 0
fare_prediction_data.loc[((fare_prediction_data['pickup_latitude']!=0) & (fare_prediction_data['pickup_longitude']!=0))&((fare_prediction_data['dropoff_latitude']==0) & (fare_prediction_data['dropoff_longitude']==0)) & (fare_prediction_data['fare_amount']==0)]

fare_prediction_data = fare_prediction_data.drop(fare_prediction_data.loc[((fare_prediction_data['pickup_latitude']!=0) & (fare_prediction_data['pickup_longitude']!=0))&((fare_prediction_data['dropoff_latitude']==0) & (fare_prediction_data['dropoff_longitude']==0)) & (fare_prediction_data['fare_amount']==0)].index, axis=0)

#3 rows dropped
fare_prediction_data.shape

#Checking fare_prediction_data data
#Again no records! AWESOME!
fare_prediction_data.loc[((fare_prediction_data['pickup_latitude']!=0) & (fare_prediction_data['pickup_longitude']!=0))&((fare_prediction_data['dropoff_latitude']==0) & (fare_prediction_data['dropoff_longitude']==0))]

"""Check the H_Distance fields which are greater than 200 kms cause there is no way that people would travel more than 200 kms at the most in NYC in a CAB!"""

high_distance = fare_prediction_data.loc[(fare_prediction_data['H_Distance']>200)&(fare_prediction_data['fare_amount']!=0)]

high_distance

"""995 rows! As you can see from the DF above, the abnormally high distances are due to either the pickup or dropoff co-ordinates being incorrect or 0. However, since all these values have fares, I do not wish to drop them as they contain crucial data. Instead, I will replace the initial distance values with distance values calculated using the fare using the following formula

> *distance = (fare_amount - 2.5)/1.56*
"""

high_distance.shape

high_distance['H_Distance'] = high_distance.apply(
    lambda row: (row['fare_amount'] - 2.50)/1.56,
    axis=1
)

#The distance values have been replaced by the newly calculated ones according to the fare
high_distance

#sync the train data with the newly computed distance values from high_distance dataframe
fare_prediction_data.update(high_distance)

fare_prediction_data.shape

"""Now we shall check for rows where the distance values are 0"""

fare_prediction_data[fare_prediction_data['H_Distance']==0]

"""We can see a few rows with distance =0. This could be due to 2 reasons
1. The cab waited the whole time and the passenger eventually cancelled. *That's why the pickup and drop co-ordinates are the same and maybe, the passenger was charged for the waiting time.*
2. The pickup and drop co-ordinates were not entered. In other words, these are **missing values**!

For the purpose of this exercise, we will drop the rows with distance value of 0
"""

fare_prediction_data = fare_prediction_data[fare_prediction_data['H_Distance']!=0]

"""## **Feature Engineering**

Feature engineering is the process of creating features - predictor variables - out of a dataset. Feature engineering is the most important step of the machine learning pipeline (A Few Useful Things to Know about Machine Learning). A model can only learn from the features it is given, and properly constructing features will determine how well your model performs.

Feature engineering involves domain expertise and experience on prior machine learning projects. A good place to start for Kaggle competitions is other kernels. Feel free to use, adapt, and build on other's work!

Relative Distances in Latitude and Longitude
As a simple first step of feature engineering, we can find the absolute value of the difference in latitude and longitude between the pickup and dropoff. While this does not represent an actual distance (we would have to convert coordinate systems), it can be used as a relative comparison of the distances of taxi rides. As the cost of a taxi ride is proportional to duration or distance, we would expect the relative distances to be a useful measure for estimating the fare.

Relative Distances in Latitude and Longitude
"""

# Absolute difference in latitude and longitude
fare_prediction_data['abs_lat_diff'] = (fare_prediction_data['dropoff_latitude'] - fare_prediction_data['pickup_latitude']).abs()
fare_prediction_data['abs_lon_diff'] = (fare_prediction_data['dropoff_longitude'] - fare_prediction_data['pickup_longitude']).abs()

"""Manhattan and Euclidean Distance
The Minkowski Distance between two points is expressed as:

D(X,Y)=(∑i=1n|xi−yi|p)1/p

if p = 1, then this is the Manhattan distance and if p = 2 this is the Euclidean distance. You may also see these referred to as the l1 or l2 norm where the number indicates p in the equation.

I should point out that these equations are only valid for actual distances in a cartesian coordinate system and here we only use them to find relative distances. To find the actual distances in terms of kilometers, we have to work with the latitude and longitude geographical coordinate system, we already used the Haversine formula.
"""

def minkowski_distance(x1, x2, y1, y2, p):
    return ((abs(x2 - x1) ** p) + (abs(y2 - y1)) ** p) ** (1 / p)

fare_prediction_data['manhattan'] = minkowski_distance(fare_prediction_data['pickup_longitude'], fare_prediction_data['dropoff_longitude'],
                                       fare_prediction_data['pickup_latitude'], fare_prediction_data['dropoff_latitude'], 1)

fare_prediction_data['euclidean'] = minkowski_distance(fare_prediction_data['pickup_longitude'], fare_prediction_data['dropoff_longitude'],
                                       fare_prediction_data['pickup_latitude'], fare_prediction_data['dropoff_latitude'], 2)

corrs = fare_prediction_data.corr()
corrs['fare_amount'].plot.bar(color = 'b');
plt.title('Correlation with Fare Amount');



"""**PART 2 --> MODELLING AND PREDICTION**

FINALLY! Data cleansing is done! Now to split the x and y variables and proceed to modelling. I shall use the random forest method for prediction
"""

from sklearn.model_selection import train_test_split

fare_prediction_data = fare_prediction_data.drop(['key','pickup_datetime','Date'], axis = 1)
# Assuming you have a feature matrix X and corresponding target variable y
X_train, X_test, y_train, y_test = train_test_split(fare_prediction_data.drop(['fare_amount'],axis=1), fare_prediction_data['fare_amount'], test_size=0.3, random_state=42)

train = pd.concat([X_train,y_train],axis=1)
test = pd.concat([X_test,y_test],axis=1)

train.columns

test.columns

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Initialize the regression models
linear_reg = LinearRegression()
decision_tree_reg = DecisionTreeRegressor()
random_forest_reg = RandomForestRegressor()
gradient_boost_reg = GradientBoostingRegressor()
knn_reg = KNeighborsRegressor()
svr_reg = SVR()
xgb_reg = XGBRegressor()

# Fit the models on the training data
linear_reg.fit(X_train, y_train)
decision_tree_reg.fit(X_train, y_train)
random_forest_reg.fit(X_train, y_train)
gradient_boost_reg.fit(X_train, y_train)
knn_reg.fit(X_train, y_train)
svr_reg.fit(X_train, y_train)
xgb_reg.fit(X_train, y_train)

# Generate predictions on the test data
linear_reg_preds = linear_reg.predict(X_test)
decision_tree_reg_preds = decision_tree_reg.predict(X_test)
random_forest_reg_preds = random_forest_reg.predict(X_test)
gradient_boost_reg_preds = gradient_boost_reg.predict(X_test)
knn_reg_preds = knn_reg.predict(X_test)
svr_reg_preds = svr_reg.predict(X_test)
xgb_reg_preds = xgb_reg.predict(X_test)

# Evaluate the performance using different metrics
linear_reg_mse = mean_squared_error(y_test, linear_reg_preds)
decision_tree_reg_mse = mean_squared_error(y_test, decision_tree_reg_preds)
random_forest_reg_mse = mean_squared_error(y_test, random_forest_reg_preds)
gradient_boost_reg_mse = mean_squared_error(y_test, gradient_boost_reg_preds)
knn_reg_mse = mean_squared_error(y_test, knn_reg_preds)
svr_reg_mse = mean_squared_error(y_test, svr_reg_preds)
xgb_reg_mse = mean_squared_error(y_test, xgb_reg_preds)

linear_reg_mae = mean_absolute_error(y_test, linear_reg_preds)
decision_tree_reg_mae = mean_absolute_error(y_test, decision_tree_reg_preds)
random_forest_reg_mae = mean_absolute_error(y_test, random_forest_reg_preds)
gradient_boost_reg_mae = mean_absolute_error(y_test, gradient_boost_reg_preds)
knn_reg_mae = mean_absolute_error(y_test, knn_reg_preds)
svr_reg_mae = mean_absolute_error(y_test, svr_reg_preds)
xgb_reg_mae = mean_absolute_error(y_test, xgb_reg_preds)

linear_reg_r2 = r2_score(y_test, linear_reg_preds)
decision_tree_reg_r2 = r2_score(y_test, decision_tree_reg_preds)
random_forest_reg_r2 = r2_score(y_test, random_forest_reg_preds)
gradient_boost_reg_r2 = r2_score(y_test, gradient_boost_reg_preds)
knn_reg_r2 = r2_score(y_test, knn_reg_preds)
svr_reg_r2 = r2_score(y_test, svr_reg_preds)
xgb_reg_r2 = r2_score(y_test, xgb_reg_preds)

# Create a DataFrame to compare the performance
results = pd.DataFrame({
    'Algorithm': ['Linear Regression', 'Decision Tree', 'Random Forest', 'Gradient Boosting',
                  'K-Nearest Neighbors', 'Support Vector Regression', 'XGBoost'],
    'MSE': [linear_reg_mse, decision_tree_reg_mse, random_forest_reg_mse, gradient_boost_reg_mse,
            knn_reg_mse, svr_reg_mse, xgb_reg_mse],
    'MAE': [linear_reg_mae, decision_tree_reg_mae, random_forest_reg_mae, gradient_boost_reg_mae,
            knn_reg_mae, svr_reg_mae, xgb_reg_mae],
    'R2 Score': [linear_reg_r2, decision_tree_reg_r2, random_forest_reg_r2, gradient_boost_reg_r2,
                 knn_reg_r2, svr_reg_r2, xgb_reg_r2]
})

# Print the results
print(results)

results.to_csv('/content/fare_prediction_results.csv', index=False)
results.head(20)